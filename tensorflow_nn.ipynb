{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d61d6-dfb2-42fb-bf78-d508f2093747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# 将数组转换为常量张量\n",
    "X = tf.cast(tf.constant(df[[\"X0\", \"X1\"]].values), tf.float32)\n",
    "y = tf.constant(df[[\"Y\"]].values)\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        # 初始化权重全为 1，也可以随机初始化\n",
    "        # 选择变量张量，因为权重后续会不断迭代更新\n",
    "        self.W1 = tf.Variable(tf.ones([2, 3]))\n",
    "        self.W2 = tf.Variable(tf.ones([3, 1]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        hidden_layer = tf.nn.sigmoid(tf.linalg.matmul(x, self.W1))  # 隐含层前向传播\n",
    "        y_ = tf.nn.sigmoid(tf.linalg.matmul(hidden_layer, self.W2))  # 输出层前向传播\n",
    "        return y_\n",
    "\n",
    "\n",
    "def loss_fn(model, X, y):\n",
    "    y_ = model(X)  # 前向传播得到预测值\n",
    "    # 使用 MSE 损失函数，并使用 reduce_mean 计算样本总损失\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(y_true=y, y_pred=y_))\n",
    "    return loss\n",
    "\n",
    "\n",
    "EPOCHS = 200  # 迭代 200 次\n",
    "LEARNING_RATE = 0.1  # 学习率\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # 使用 GradientTape 追踪梯度\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, X, y)  # 计算 Loss，包含前向传播过程\n",
    "    # 使用梯度下降法优化迭代\n",
    "    # 输出模型需优化参数 W1，W2 自动微分结果\n",
    "    dW1, dW2 = tape.gradient(loss, [model.W1, model.W2])\n",
    "    model.W1.assign_sub(LEARNING_RATE * dW1)  # 更新梯度\n",
    "    model.W2.assign_sub(LEARNING_RATE * dW2)\n",
    "\n",
    "    # 每 100 个 Epoch 输出各项指标\n",
    "    if epoch == 0:\n",
    "        print(f\"Epoch [000/{EPOCHS}], Loss: [{loss:.4f}]\")\n",
    "    elif (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: [{loss:.4f}]\")\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "\n",
    "loss_list = []  # 存放每一次 loss\n",
    "model = Model()  # 实例化类\n",
    "for epoch in range(EPOCHS):\n",
    "    # 使用 GradientTape 追踪梯度\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, X, y)  # 计算 Loss，包含前向传播过程\n",
    "        loss_list.append(loss)  # 保存每次迭代 loss\n",
    "\n",
    "    grads = tape.gradient(loss, [model.W1, model.W2])  # 输出自动微分结果\n",
    "    optimizer.apply_gradients(zip(grads, [model.W1, model.W2]))  # 使用优化器更新梯度\n",
    "\n",
    "    # 每 100 个 Epoch 输出各项指标\n",
    "    if epoch == 0:\n",
    "        print(f\"Epoch [000/{EPOCHS}], Loss: [{loss:.4f}]\")\n",
    "    elif (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: [{loss:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
